{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec22c4af",
   "metadata": {},
   "source": [
    "# Lectures 7 and 8: Building Autograd engine from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5044c4c",
   "metadata": {},
   "source": [
    "## Learning outcomes\n",
    "1. Understanding automated differentiatiom engines at a foundational level\n",
    "2. Operator overloading in OOP\n",
    "3. Graph representation\n",
    "4. Graph traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9b460",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Restrictin gyourself to <font color='red'>Python's Standard Library</font>, build an <font color='red'>autograd engine</font> capable of estimating the gradients required to solve the following problem using gradient descent.\n",
    "\n",
    "Find a point in the R<sup>2</sup> with the least average Euclidean distance to a set of arbitrary points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cef787",
   "metadata": {},
   "source": [
    "**As long as my loss is scalar differentiable wrt my unknowns the methods that will be explained in this lecture are applicable to any problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa232fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "from math import sqrt, ceil\n",
    "SEED = 5\n",
    "\n",
    "def generate_pnts(N = 1000):\n",
    "    random_gen = Random(x = SEED)\n",
    "    lst_x , lst_y =[],[]\n",
    "    for _ in range(N):\n",
    "        lst_x.append(random_gen.uniform(a = 0, b = 1))\n",
    "    for _ in range(N):\n",
    "        lst_y.append(random_gen.uniform(a = 0, b = 1))\n",
    "    return lst_x, lst_y\n",
    "\n",
    "# closed form analytically driven calculation of the gradient\n",
    "def calc_grad(x_p, y_p, x_i, y_i):\n",
    "    sum_x, sum_y = 0, 0\n",
    "    n = len(x_i)\n",
    "    for x, y in zip(x_i, y_i):\n",
    "        inv_sqrt = ((x - x_p) ** 2 + (y - y_p) ** 2) ** (-0.5)\n",
    "        sum_x += inv_sqrt * (x - x_p)\n",
    "        sum_y += inv_sqrt * (y - y_p)\n",
    "    return -sum_x/n, -sum_y/n\n",
    "\n",
    "def loss(x_p, y_p, x_i, y_i):\n",
    "  n = len(x_i)\n",
    "  return (1 / n) * sum(\n",
    "      [sqrt((x_i - x_p)**2 + (y_i - y_p)**2)\n",
    "        for x_i, y_i in zip(x_i, y_i)\n",
    "      ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3edeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed form: gradient for x_p -0.3277744397151291, gradient for y_p -0.336282174741702 \n",
      "Closed form: loss = 0.4430528244756474\n",
      "Closed form: gradient for x_p -0.5900849147094943, gradient for y_p -0.8073411877467226 \n",
      "Closed form: loss = 0.5472122517293468\n"
     ]
    }
   ],
   "source": [
    "data_x, data_y = generate_pnts()\n",
    "x_p, y_p = 0.3, 0.3\n",
    "grad_x, grad_y = calc_grad(x_p, y_p, data_x, data_y)\n",
    "cur_loss = loss(x_p, y_p, data_x, data_y)\n",
    "print(f\"Closed form: gradient for x_p {grad_x}, gradient for y_p {grad_y} \")\n",
    "print(f\"Closed form: loss = {cur_loss}\")\n",
    "\n",
    "#loss for one pnt to compare to the autograd at the end (forward pass)\n",
    "data_x, data_y = generate_pnts(N = 1)\n",
    "x_p, y_p = 0.3, 0.3\n",
    "grad_x, grad_y = calc_grad(x_p, y_p, data_x, data_y)\n",
    "cur_loss = loss(x_p, y_p, data_x, data_y)\n",
    "print(f\"Closed form: gradient for x_p {grad_x}, gradient for y_p {grad_y} \")\n",
    "print(f\"Closed form: loss = {cur_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5ba99",
   "metadata": {},
   "source": [
    "## Pytorch as an example of autograd engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde234dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "torch.Size([1, 2]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data_pnt = torch.tensor([0.3, 0.3])\n",
    "data_pnt.requires_grad = True\n",
    "data_pnt.retain_grad()\n",
    "data = torch.tensor([data_x, data_y])\n",
    "print(data.shape)\n",
    "data = data.t()\n",
    "print(data.shape, data_pnt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1d0c3",
   "metadata": {},
   "source": [
    "**Why do I need to zero the grad?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1856fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss calc by pytorch: 0.5472122430801392\n",
      "torch grad: tensor([-0.5901, -0.8073])\n",
      "**************************************************\n",
      "loss calc by pytorch: 0.5472122430801392\n",
      "torch grad: tensor([-1.1802, -1.6147])\n",
      "**************************************************\n",
      "loss calc by pytorch: 0.5472122430801392\n",
      "torch grad: tensor([-1.7703, -2.4220])\n",
      "torch grad after zeroing: tensor([0., 0.])\n",
      "loss calc by pytorch: 0.5472122430801392\n",
      "torch grad: tensor([-0.5901, -0.8073])\n",
      "torch grad after zeroing: tensor([0., 0.])\n",
      "loss calc by pytorch: 0.5472122430801392\n",
      "torch grad: tensor([-0.5901, -0.8073])\n",
      "torch grad after zeroing: tensor([0., 0.])\n",
      "loss calc by pytorch: 0.5472122430801392\n",
      "torch grad: tensor([-0.5901, -0.8073])\n",
      "torch grad after zeroing: tensor([0., 0.])\n",
      "loss calc by pytorch: 0.5472122430801392\n",
      "torch grad: tensor([-0.5901, -0.8073])\n",
      "torch grad after zeroing: tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    loss_torch = torch.mean(torch.sqrt(((data - data_pnt)**2).sum(dim=1))) #foward pass\n",
    "    print(f\"loss calc by pytorch: {loss_torch}\")\n",
    "    #every .backward the gradient is accumulated aka added \n",
    "    #it is automated it doesnt know that we are repeating the row\n",
    "    # it treats it as a new dtaa_pnt so it calcs gard and accumulates them\n",
    "    #REMEMBER -> while calc the deriveative aka gradient wrt all points we ADDED THEM aka accumulated them that is what it is\n",
    "    # doing here\n",
    "    # first .backward()->gard then 2nd-> 2*grad then 3rd->3*grad and so on \n",
    "    # the lower one here is 2*grad and 3*grad\n",
    "    loss_torch.backward()\n",
    "    print(f\"torch grad: {data_pnt.grad.data}\")\n",
    "    print('*' *50)\n",
    "    #ZEROING\n",
    "for _ in range(5):\n",
    "    loss_torch = torch.mean(torch.sqrt(((data - data_pnt)**2).sum(dim=1)))\n",
    "    print(f\"loss calc by pytorch: {loss_torch}\")\n",
    "    #every .backward the gradient is accumulated aka added \n",
    "    #it is automated it doesnt know that we are repeating the row\n",
    "    # it treats it as a new dtaa_pnt so it calcs gard and accumulates them\n",
    "    #REMEMBER -> while calc the deriveative aka gradient wrt all points we ADDED THEM aka accumulated them that is what it is\n",
    "    # doing here\n",
    "    # first .backward()->grad then 2nd-> 2*grad then 3rd->3*grad and so on \n",
    "    # the lower one here is 2*grad and 3*grad\n",
    "    loss_torch.backward()\n",
    "    print(f\"torch grad: {data_pnt.grad.data}\")\n",
    "    data_pnt.grad.zero_() #inplace\n",
    "    print(f\"torch grad after zeroing: {data_pnt.grad.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e0827",
   "metadata": {},
   "source": [
    "**What is my loss relative to my data -> forward pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44a6baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss calc by pytorch: 0.44305282831192017\n"
     ]
    }
   ],
   "source": [
    "#broadcasting therefore element-wise subtraction\n",
    "#this is the forward pass and pytorch could calc the back grad (the next code cell)\n",
    "loss_torch = torch.mean(torch.sqrt(((data - data_pnt)**2).sum(dim=1)))\n",
    "#dim=0 -> across rows, dim=1 ->across columns\n",
    "print(f\"loss calc by pytorch: {loss_torch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f2269",
   "metadata": {},
   "source": [
    "**Notice that it is equivalent to the closed form of the loss above**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cc710",
   "metadata": {},
   "source": [
    "Which variable do i need to calc grad wr to? pnt therefore require and retain grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "decee7df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch grad: tensor([-0.3278, -0.3363])\n"
     ]
    }
   ],
   "source": [
    "loss_torch.backward()\n",
    "print(f\"torch grad: {data_pnt.grad.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6502d",
   "metadata": {},
   "source": [
    "## Backpropagation and graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed3296",
   "metadata": {},
   "source": [
    "![grads1](grads1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4c454",
   "metadata": {},
   "source": [
    "![nodesimg](nodes2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5d6bc",
   "metadata": {},
   "source": [
    "Initialization dl0/dlo = 1\n",
    "reverse topological sort-> i vists every node(parent) before its children\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13907cc5",
   "metadata": {},
   "source": [
    "grad2 += gard1 * local_gradient\n",
    "grad1 is the parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd584af",
   "metadata": {},
   "source": [
    "## Building autograd from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffc75794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val -> value calc in forward pass\n",
    "#grad initilaize to zero bcus it is the additive neutral (since I accumulate the grad)\n",
    "#self-driven code -> bakteb eltest abl elcode\n",
    "\n",
    "class comp_node:\n",
    "    \n",
    "    def __init__(self, val, children = [], op = \"assign\"):\n",
    "        self.val = val\n",
    "        self.children = children\n",
    "        self.grad = 0\n",
    "        self.op = op\n",
    "        self.backward_prop = lambda : None\n",
    "        \n",
    "    def __to_comp_node(self, obj):\n",
    "        if not isinstance(obj, comp_node):                 \n",
    "            return comp_node(val = obj)\n",
    "        else:\n",
    "            return obj\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        \n",
    "       # if not isinstance(other, comp_node):\n",
    "        #    other = comp_node(val = other)\n",
    "        other = self.__to_comp_node(other)\n",
    "        out =  comp_node(val = (self.val - other.val), \n",
    "                         children = [self, other], op = \"subtract\")\n",
    "        return out\n",
    "    \n",
    "    #here self became the right operand\n",
    "    #default was the left\n",
    "    def __rsub__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        return other - self\n",
    "     #  # if not isinstance(other, comp_node):\n",
    "     #   #    other = comp_node(val = other)\n",
    "     #   other = self.__to_comp_node(other)\n",
    "     #   out =  comp_node(val = (other.val - self.val), \n",
    "     #                    children = [self, other], op = \"sub\")\n",
    "        \n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        if not isinstance(exponent, (int, float)):\n",
    "            raise ValueError(\"Unsupported types of exponent\")\n",
    "        out = comp_node(val = self.val ** exponent, children = [self], op=f\"power {exponent}\")\n",
    "        # I do not understand this line \n",
    "        #the one with the underscore is a different function\n",
    "        def _backward_prop():\n",
    "            self.grad += out.grad * (exponent * self.val * (exponent - 1))\n",
    "        out.backward_prop = _backward_prop()\n",
    "        return out\n",
    "    \n",
    "    #think about do i have to compare the children as well\n",
    "    #ACCORDING TO THE APPLICATION\n",
    "    def __eq__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        return self.val == other.val\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        out = comp_node(val = self.val + other.val, children = [self, other], op = \"add\")\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        return self +other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        out = comp_node(val = self.val * other.val, children = [self, other], op = \"mul\")\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        return self*other\n",
    "    \n",
    "    #This was the output when we printed a nde <__main__.comp_node object at 0x00000206159DA230>\n",
    "    #we will overload represent aka toString in java\n",
    "    # .4f -> 4 places after the decimal\n",
    "    def __repr__(self):\n",
    "        return f\"op = {self.op} | val = {self.val:.4f} | children = {len(self.children)} | grad = {self.grad}\"\n",
    "    \n",
    "assert comp_node(val = 5).val == 5, \"Assignment failed\"\n",
    "assert (comp_node(val = 5) - comp_node(val = 3)).val == 2       \n",
    "assert (comp_node(val = 5) - 3).val == 2\n",
    "assert (3 - comp_node(val = 2)).val == 1\n",
    "assert (comp_node(val = 7) - comp_node(val = 2)).val == 5\n",
    "\n",
    "#we didnt tell comp node how to do the comparison, we must overload it\n",
    "#therefore __eq__\n",
    "assert (comp_node(val = 5) ** 2) == comp_node(val = 25)\n",
    "\n",
    "#this one was here by default\n",
    "assert comp_node(val = 3) + comp_node(val = 5) == comp_node(val = 8)\n",
    "\n",
    "assert comp_node(val = 3) + 3 == 6\n",
    "assert (3 + comp_node(val = 3)).val == 6\n",
    "\n",
    "assert (comp_node(val = 3) *2).val == 6\n",
    "assert comp_node(val = 3) *comp_node(val = 3) == comp_node(val = 9)\n",
    "\n",
    "assert(comp_node(val = 3) *2).val == 6\n",
    "assert(2 * comp_node(val = 3)).val == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d0bde",
   "metadata": {},
   "source": [
    "**Let's begin with our forward pass for the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "842ac836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op = power 0.5 | val = 0.5472 | children = 1 | grad = 0\n"
     ]
    }
   ],
   "source": [
    "data_x, data_y = generate_pnts(N = 1)\n",
    "x_p, y_p = comp_node(val = 0.3), comp_node(val = 0.3)\n",
    "\n",
    "def loss_graph(x_p, y_p, data_x, data_y):\n",
    "    I_x, I_y = x_p - data_x, y_p - data_y\n",
    "    g_x, g_y = I_x**2, I_y**2\n",
    "    M = g_x + g_y\n",
    "    l = M ** 0.5\n",
    "    #we will also return the nodes in inverse topological sort\n",
    "    return l, [l, M, g_x, g_y, I_x, I_y, x_p, y_p]\n",
    "\n",
    "#data_x and data_y are lists therefore we must use data\n",
    "#_x[0] (in our simple example, we are using one point)\n",
    "\n",
    "l, rev_topo_order = loss_graph(x_p, y_p, data_x[0], data_y[0])\n",
    "print(l)\n",
    "\n",
    "#Notice tha it is the same as the last cell before autograd engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31bd29",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "* reverse topological sort of our nodes\n",
    "* initialization of dlo/dlo = 1\n",
    "* apply chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1ae31d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op = power 0.5 | val = 0.5472 | children = 1 | grad = 0\n",
      "0 op = power 0.5 | val = 0.5472 | children = 1 | grad = 1\n",
      "1 op = add | val = 0.2994 | children = 2 | grad = 0.0\n",
      "2 op = power 2 | val = 0.1043 | children = 1 | grad = 0\n",
      "3 op = power 2 | val = 0.1952 | children = 1 | grad = 0\n",
      "4 op = subtract | val = -0.3229 | children = 2 | grad = 0.0\n",
      "5 op = subtract | val = -0.4418 | children = 2 | grad = 0.0\n",
      "6 op = assign | val = 0.3000 | children = 0 | grad = 0\n",
      "7 op = assign | val = 0.3000 | children = 0 | grad = 0\n"
     ]
    }
   ],
   "source": [
    "data_x, data_y = generate_pnts(N = 1)\n",
    "x_p, y_p = comp_node(val = 0.3), comp_node(val = 0.3)\n",
    "\n",
    "def loss_graph(x_p, y_p, data_x, data_y):\n",
    "    I_x, I_y = x_p - data_x, y_p - data_y\n",
    "    g_x, g_y = I_x**2, I_y**2\n",
    "    M = g_x + g_y\n",
    "    l = M ** 0.5\n",
    "    return l\n",
    "\n",
    "#data_x and data_y are lists therefore we must use data\n",
    "#_x[0] (in our simple example, we are using one point)\n",
    "\n",
    "l = loss_graph(x_p, y_p, data_x[0], data_y[0])\n",
    "# grad initilization\n",
    "rev_topo_order[0].grad = 1\n",
    "print(l)\n",
    "#Notice tha it is the same as the last cell before autograd engine\n",
    "\n",
    "for i, node in enumerate(rev_topo_order):\n",
    "    print(i, node)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc1e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
